% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizer.R
\name{nlp_tokenizer}
\alias{nlp_tokenizer}
\title{Spark NLP Tokenizer approach}
\usage{
nlp_tokenizer(x, input_cols, output_col, exceptions = NULL,
  exceptions_path = NULL, exceptions_path_read_as = "LINE_BY_LINE",
  exceptions_path_options = list(format = "text"),
  case_sensitive_exceptions = NULL, context_chars = NULL,
  split_chars = NULL, split_pattern = NULL, target_pattern = NULL,
  suffix_pattern = NULL, prefix_pattern = NULL,
  infix_patterns = NULL, uid = random_string("tokenizer_"))
}
\arguments{
\item{x}{A \code{spark_connection}, \code{ml_pipeline}, or a \code{tbl_spark}.}

\item{input_cols}{Input columns. String array.}

\item{output_col}{Output column. String.}

\item{exceptions}{String array. List of tokens to not alter at all. Allows composite tokens like two worded tokens that the user may not want to split.}

\item{exceptions_path}{NOTE: NOT IMPLEMENTED. String. Path to txt file with list of token exceptions}

\item{exceptions_path_read_as}{LINE_BY_LINE or SPARK_DATASET}

\item{exceptions_path_options}{Options to pass to the Spark reader. Defaults to {"format" = "text"}}

\item{case_sensitive_exceptions}{Boolean. Whether to follow case sensitiveness for matching exceptions in text}

\item{context_chars}{String array. Whether to follow case sensitiveness for matching exceptions in text}

\item{split_chars}{String array.  List of 1 character string to rip off from tokens, such as parenthesis or question marks. Ignored if using prefix, infix or suffix patterns.}

\item{split_pattern}{String. pattern to separate from the inside of tokens. takes priority over splitChars.}

\item{target_pattern}{String. Basic regex rule to identify a candidate for tokenization. Defaults to `\\S+` which means anything not a space}

\item{suffix_pattern}{String. Regex to identify subtokens that are in the end of the token. Regex has to end with `\\z` and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis}

\item{prefix_pattern}{String. Regex to identify subtokens that come in the beginning of the token. Regex has to start with `\\A` and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis}

\item{infix_patterns}{String array. extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).}

\item{uid}{A character string used to uniquely identify the ML estimator.}

\item{...}{Optional arguments, see Details.}
}
\value{
The object returned depends on the class of \code{x}.

\itemize{
  \item \code{spark_connection}: When \code{x} is a \code{spark_connection}, the function returns an instance of a \code{ml_estimator} object. The object contains a pointer to
  a Spark \code{Estimator} object and can be used to compose
  \code{Pipeline} objects.

  \item \code{ml_pipeline}: When \code{x} is a \code{ml_pipeline}, the function returns a \code{ml_pipeline} with
  the NLP estimator appended to the pipeline.

  \item \code{tbl_spark}: When \code{x} is a \code{tbl_spark}, an estimator is constructed then
  immediately fit with the input \code{tbl_spark}, returning an NLP model.
}
}
\description{
Spark ML estimator that identifies tokens with tokenization open standards. A few rules will help customizing
it if defaults do not fit user needs. See \url{https://nlp.johnsnowlabs.com/docs/en/annotators#tokenizer}
}
